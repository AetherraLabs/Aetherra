#!/usr/bin/env python3
"""
Test Ollama Configuration for Aetherra
"""

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_ollama_installation():
    """Test if Ollama is properly installed and accessible"""

    print("üîç Testing Ollama Configuration...")
    print("=" * 50)

    # Test 1: Check if Ollama Python package is installed
    try:
        import ollama

        print("‚úÖ Ollama Python package installed")
    except ImportError as e:
        print(f"‚ùå Ollama Python package not installed: {e}")
        print("   Run: pip install ollama")
        return False

    # Test 2: Check if Ollama service is running
    try:
        client = ollama.Client()
        models = client.list()
        print("‚úÖ Ollama service is running")
        print(f"   Service URL: http://localhost:11434")
    except Exception as e:
        print(f"‚ùå Ollama service not accessible: {e}")
        print("   Make sure Ollama is installed and running:")
        print("   - Download from: https://ollama.ai/download")
        print("   - Or run: ollama serve")
        return False

    # Test 3: Check available models
    try:
        available_models = models.get("models", [])
        if available_models:
            print(f"‚úÖ Found {len(available_models)} Ollama models:")
            for model in available_models:
                model_name = model.get("name", "unknown")
                size = model.get("size", 0) / (1024**3)  # Convert to GB
                print(f"   - {model_name} ({size:.1f}GB)")
        else:
            print("‚ö†Ô∏è  No models downloaded yet")
            print("   Download a model with: ollama pull llama2:7b")
            return False
    except Exception as e:
        print(f"‚ùå Error listing models: {e}")
        return False

    # Test 4: Test model response
    try:
        # Use the first available model for testing
        test_model = available_models[0]["name"].split(":")[0]
        print(f"\nüß™ Testing model '{test_model}'...")

        response = client.chat(
            model=test_model,
            messages=[
                {
                    "role": "user",
                    "content": "Hello! Can you respond with just 'Ollama is working'?",
                }
            ],
            options={"temperature": 0.1, "num_predict": 10},
        )

        response_text = response["message"]["content"].strip()
        print(f"‚úÖ Model response: {response_text}")

    except Exception as e:
        print(f"‚ùå Error testing model: {e}")
        return False

    print("\nüéâ Ollama is fully configured and ready!")
    return True


def test_aetherra_integration():
    """Test if Aetherra can detect and use Ollama"""

    print("\nüîç Testing Aetherra Integration...")
    print("=" * 50)

    try:
        # Import Aetherra's MultiLLMManager
        from Aetherra.core.ai.multi_llm_manager import LLMProvider, MultiLLMManager

        # Initialize the manager
        manager = MultiLLMManager()

        # Check if Ollama provider is available
        if LLMProvider.OLLAMA in manager.providers:
            print("‚úÖ Aetherra detected Ollama provider")

            # List available models through Aetherra
            available_models = manager.list_available_models()
            ollama_models = [
                name
                for name, config in available_models.items()
                if config.provider == LLMProvider.OLLAMA
            ]

            if ollama_models:
                print(f"‚úÖ Available Ollama models in Aetherra: {ollama_models}")

                # Test setting an Ollama model
                test_model = ollama_models[0]
                if manager.set_model(test_model):
                    print(f"‚úÖ Successfully set model: {test_model}")
                else:
                    print(f"‚ùå Failed to set model: {test_model}")

            else:
                print("‚ö†Ô∏è  No Ollama models available in Aetherra")

        else:
            print("‚ùå Aetherra did not detect Ollama provider")
            return False

    except ImportError as e:
        print(f"‚ùå Could not import Aetherra components: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Error testing Aetherra integration: {e}")
        return False

    print("üéâ Aetherra integration successful!")
    return True


if __name__ == "__main__":
    print("üöÄ Ollama Configuration Test for Aetherra")
    print("=" * 50)

    # Run tests
    ollama_ok = test_ollama_installation()
    if ollama_ok:
        aetherra_ok = test_aetherra_integration()

        if ollama_ok and aetherra_ok:
            print("\nüåü ALL TESTS PASSED!")
            print("Ollama is ready to use with Lyrixa as a fallback model.")
            print("\nNow when OpenAI models fail, Lyrixa will automatically")
            print("fall back to your local Ollama models! üéØ")
        else:
            print("\n‚ö†Ô∏è  Some tests failed. Check the errors above.")
    else:
        print("\n‚ùå Ollama setup incomplete. Please fix the issues above.")
