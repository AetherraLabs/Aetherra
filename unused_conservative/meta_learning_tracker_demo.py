#!/usr/bin/env python3
"""
üß† META-LEARNING TRACKER DEMONSTRATION
=====================================

Demonstrates the enhanced Meta-Learning Tracker capabilities:

üìö log_learning_session(context, outcome, time_spent): Log and analyze learning sessions
üìä score_effectiveness(): Evaluate learning success via confidence, contradiction reduction, narrative clarity
üéØ recommend_strategy_updates(): Suggest retry, change tools, or escalate based on performance

Key Features:
‚Ä¢ Comprehensive learning session logging with meta-analysis
‚Ä¢ Multi-dimensional effectiveness scoring and grading
‚Ä¢ Intelligent strategy recommendations with escalation triggers
‚Ä¢ Method and agent performance analysis
‚Ä¢ Adaptive learning optimization suggestions
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path

# Import the enhanced learning loop integration agent
try:
    from Aetherra.lyrixa.agents.learning_loop_integration_agent import (
        LearningLoopIntegrationAgent,
    )
except ImportError:
    print("‚ö†Ô∏è Using local import paths for demo")
    import sys

    sys.path.append(".")
    from learning_loop_integration_agent import LearningLoopIntegrationAgent


class MetaLearningTrackerDemo:
    """
    Comprehensive demonstration of Meta-Learning Tracker capabilities
    """

    def __init__(self):
        self.agent = LearningLoopIntegrationAgent(data_dir="meta_learning_data")
        self.demo_results = {
            "learning_sessions": [],
            "effectiveness_scores": [],
            "strategy_recommendations": [],
        }

    async def run_comprehensive_demo(self):
        """Run the complete Meta-Learning Tracker demonstration"""
        print("üß† META-LEARNING TRACKER DEMONSTRATION")
        print("=" * 70)
        print("Evaluate how well Lyrixa learns ‚Äî and adapt strategies accordingly")
        print("=" * 70)

        # Initialize the agent
        await self.agent.initialize()

        # Phase 1: Log diverse learning sessions
        await self.demonstrate_session_logging()

        # Phase 2: Score learning effectiveness
        await self.demonstrate_effectiveness_scoring()

        # Phase 3: Generate strategy recommendations
        await self.demonstrate_strategy_recommendations()

        # Phase 4: Show adaptive learning insights
        await self.demonstrate_adaptive_insights()

        # Phase 5: Generate comprehensive report
        await self.generate_meta_learning_report()

    async def demonstrate_session_logging(self):
        """Demonstrate log_learning_session() with diverse scenarios"""
        print("\nüìö PHASE 1: COMPREHENSIVE LEARNING SESSION LOGGING")
        print("=" * 65)

        # Define diverse learning sessions to simulate
        learning_sessions = [
            {
                "context": {
                    "goal_id": "async_programming_goal",
                    "learning_method": "experimentation",
                    "agent_used": "curiosity_agent",
                    "initial_state": {"confidence": 0.3, "knowledge_level": "beginner"},
                    "target_objective": "Understand async/await patterns",
                    "resources_used": ["documentation", "code_examples", "tutorials"],
                    "difficulty_level": "high",
                },
                "outcome": {
                    "success_level": "breakthrough",
                    "knowledge_gained": "Deep understanding of async patterns and event loops",
                    "confidence_change": 0.6,
                    "contradiction_reduction": 2,
                    "narrative_clarity_improvement": 0.8,
                    "unexpected_discoveries": [
                        "Event loop optimization patterns",
                        "Memory management insights",
                    ],
                    "obstacles_encountered": [
                        "Initial confusion with coroutines",
                        "Debugging async code",
                    ],
                    "final_state": {"confidence": 0.9, "knowledge_level": "advanced"},
                },
                "time_spent": 120.0,
            },
            {
                "context": {
                    "goal_id": "user_feedback_contradiction",
                    "learning_method": "reflection",
                    "agent_used": "contradiction_detector",
                    "initial_state": {"confidence": 0.5, "understanding": "conflicted"},
                    "target_objective": "Resolve user preference contradictions",
                    "resources_used": ["interaction_logs", "feedback_analysis"],
                    "difficulty_level": "medium",
                },
                "outcome": {
                    "success_level": "success",
                    "knowledge_gained": "Context-dependent user preferences understanding",
                    "confidence_change": 0.4,
                    "contradiction_reduction": 1,
                    "narrative_clarity_improvement": 0.6,
                    "unexpected_discoveries": ["Temporal preference shifts"],
                    "obstacles_encountered": ["Conflicting evidence interpretation"],
                    "final_state": {"confidence": 0.9, "understanding": "resolved"},
                },
                "time_spent": 45.0,
            },
            {
                "context": {
                    "goal_id": "debugging_consistency",
                    "learning_method": "observation",
                    "agent_used": "self_evaluation_agent",
                    "initial_state": {"confidence": 0.4, "consistency": "variable"},
                    "target_objective": "Improve debugging approach consistency",
                    "resources_used": [
                        "error_logs",
                        "debugging_sessions",
                        "pattern_analysis",
                    ],
                    "difficulty_level": "low",
                },
                "outcome": {
                    "success_level": "partial",
                    "knowledge_gained": "Pattern recognition in debugging approaches",
                    "confidence_change": 0.2,
                    "contradiction_reduction": 0,
                    "narrative_clarity_improvement": 0.3,
                    "unexpected_discoveries": [],
                    "obstacles_encountered": ["Inconsistent debugging results"],
                    "final_state": {"confidence": 0.6, "consistency": "improving"},
                },
                "time_spent": 30.0,
            },
            {
                "context": {
                    "goal_id": "memory_optimization",
                    "learning_method": "research",
                    "agent_used": "learning_loop_integration",
                    "initial_state": {"confidence": 0.1, "knowledge": "minimal"},
                    "target_objective": "Learn advanced memory optimization",
                    "resources_used": [
                        "academic_papers",
                        "performance_benchmarks",
                        "case_studies",
                    ],
                    "difficulty_level": "extreme",
                },
                "outcome": {
                    "success_level": "failure",
                    "knowledge_gained": "Basic memory optimization concepts",
                    "confidence_change": 0.1,
                    "contradiction_reduction": 0,
                    "narrative_clarity_improvement": 0.1,
                    "unexpected_discoveries": [],
                    "obstacles_encountered": [
                        "Complex algorithms",
                        "Mathematical complexity",
                        "Implementation challenges",
                    ],
                    "final_state": {"confidence": 0.2, "knowledge": "basic"},
                },
                "time_spent": 180.0,
            },
        ]

        print(f"üìö Logging {len(learning_sessions)} diverse learning sessions...")
        print("üéØ Session types: Breakthrough, Success, Partial, Failure")
        print("üî¨ Methods: Experimentation, Reflection, Observation, Research")
        print("‚öñÔ∏è Difficulty levels: Low ‚Üí Extreme")

        logged_sessions = []
        for i, session_data in enumerate(learning_sessions, 1):
            context = session_data["context"]
            outcome = session_data["outcome"]
            time_spent = session_data["time_spent"]

            print(f"\nüìù Logging Session {i}: {context['goal_id']}")
            print(f"   üî¨ Method: {context['learning_method']}")
            print(f"   ü§ñ Agent: {context['agent_used']}")
            print(f"   ‚è±Ô∏è Duration: {time_spent} minutes")
            print(f"   üéØ Success: {outcome['success_level']}")
            print(f"   üìà Confidence Change: {outcome['confidence_change']:+.1f}")

            # Log the learning session
            session_id = await self.agent.log_learning_session(
                context, outcome, time_spent
            )
            logged_sessions.append(session_id)

            print(f"   ‚úÖ Logged as: {session_id}")

            self.demo_results["learning_sessions"].append(
                {
                    "session_id": session_id,
                    "goal_id": context["goal_id"],
                    "method": context["learning_method"],
                    "success_level": outcome["success_level"],
                    "time_spent": time_spent,
                    "confidence_change": outcome["confidence_change"],
                }
            )

        print(f"\n‚úÖ Successfully logged {len(logged_sessions)} learning sessions!")
        print(
            "üìä Sessions now available for effectiveness analysis and strategy recommendations"
        )
        return logged_sessions

    async def demonstrate_effectiveness_scoring(self):
        """Demonstrate score_effectiveness() with comprehensive analysis"""
        print("\nüìä PHASE 2: LEARNING EFFECTIVENESS SCORING")
        print("=" * 60)

        print("üìä Analyzing learning effectiveness across all dimensions...")
        print(
            "üéØ Metrics: Confidence improvement, contradiction reduction, narrative clarity"
        )
        print("üìà Analysis: Method performance, agent effectiveness, learning trends")

        # Score effectiveness for all recent sessions
        effectiveness_analysis = await self.agent.score_effectiveness(
            timeframe_hours=24.0
        )

        print(f"\n‚úÖ **EFFECTIVENESS ANALYSIS RESULTS**:")
        print(f"‚Ä¢ Sessions Analyzed: {effectiveness_analysis['sessions_analyzed']}")
        print(
            f"‚Ä¢ Overall Effectiveness: {effectiveness_analysis['overall_effectiveness']:.2f}"
        )
        print(f"‚Ä¢ Effectiveness Grade: {effectiveness_analysis['effectiveness_grade']}")
        print(f"‚Ä¢ Timeframe: {effectiveness_analysis['timeframe_hours']} hours")

        # Show core metrics
        core_metrics = effectiveness_analysis["core_metrics"]
        print(f"\nüìä **CORE LEARNING METRICS**:")
        print(
            f"‚Ä¢ Total Confidence Change: {core_metrics['total_confidence_change']:+.1f}"
        )
        print(
            f"‚Ä¢ Total Contradiction Reduction: {core_metrics['total_contradiction_reduction']}"
        )
        print(
            f"‚Ä¢ Total Narrative Improvement: {core_metrics['total_narrative_improvement']:.1f}"
        )
        print(
            f"‚Ä¢ Average Session Time: {core_metrics['average_session_time']:.1f} minutes"
        )

        # Show method analysis
        method_analysis = effectiveness_analysis["method_analysis"]
        if method_analysis:
            print(f"\nüî¨ **METHOD PERFORMANCE ANALYSIS**:")
            for method, stats in method_analysis.items():
                print(f"   {method.title()}:")
                print(f"      ‚Ä¢ Sessions: {stats['session_count']}")
                print(f"      ‚Ä¢ Effectiveness: {stats['average_effectiveness']:.2f}")
                print(f"      ‚Ä¢ Success Rate: {stats['success_rate']:.2f}")
                print(f"      ‚Ä¢ Avg Time: {stats['average_time']:.1f} min")
                print(f"      ‚Ä¢ Breakthrough Rate: {stats['breakthrough_rate']:.2f}")

        # Show agent analysis
        agent_analysis = effectiveness_analysis["agent_analysis"]
        if agent_analysis:
            print(f"\nü§ñ **AGENT PERFORMANCE ANALYSIS**:")
            for agent, stats in agent_analysis.items():
                print(f"   {agent}:")
                print(f"      ‚Ä¢ Sessions: {stats['session_count']}")
                print(f"      ‚Ä¢ Effectiveness: {stats['average_effectiveness']:.2f}")
                print(f"      ‚Ä¢ Confidence Impact: {stats['confidence_impact']:+.2f}")
                print(f"      ‚Ä¢ Speed Score: {stats['speed_score']:.2f}")

        # Show effectiveness trends
        trends = effectiveness_analysis["effectiveness_trends"]
        print(f"\nüìà **LEARNING EFFECTIVENESS TRENDS**:")
        print(
            f"‚Ä¢ Confidence Improvement Rate: {trends['confidence_improvement_rate']:+.2f} per session"
        )
        print(
            f"‚Ä¢ Contradiction Resolution Rate: {trends['contradiction_resolution_rate']:.2f} per session"
        )
        print(
            f"‚Ä¢ Narrative Clarity Rate: {trends['narrative_clarity_improvement_rate']:.2f} per session"
        )
        print(f"‚Ä¢ Learning Velocity Trend: {trends['learning_velocity_trend']:.2f}")
        print(
            f"‚Ä¢ Difficulty Adaptation Score: {trends['difficulty_adaptation_score']:.2f}"
        )

        # Show improvement areas
        improvement_areas = effectiveness_analysis["improvement_areas"]
        if improvement_areas:
            print(f"\nüöÄ **IMPROVEMENT AREAS IDENTIFIED**:")
            for area in improvement_areas:
                print(f"   ‚Ä¢ {area}")

        # Show learning insights
        learning_insights = effectiveness_analysis["learning_insights"]
        if learning_insights:
            print(f"\nüí° **META-LEARNING INSIGHTS**:")
            for insight in learning_insights:
                print(f"   ‚Ä¢ {insight}")

        self.demo_results["effectiveness_scores"].append(effectiveness_analysis)
        return effectiveness_analysis

    async def demonstrate_strategy_recommendations(self):
        """Demonstrate recommend_strategy_updates() with intelligent suggestions"""
        print("\nüéØ PHASE 3: INTELLIGENT STRATEGY RECOMMENDATIONS")
        print("=" * 65)

        print(
            "üéØ Analyzing learning patterns and generating strategy recommendations..."
        )
        print("‚öñÔ∏è Evaluation: Whether to retry, change tools, or escalate")
        print("üöÄ Focus: Method optimization, agent configuration, escalation triggers")

        # Get latest effectiveness analysis
        latest_effectiveness = (
            self.demo_results["effectiveness_scores"][-1]
            if self.demo_results["effectiveness_scores"]
            else None
        )

        # Generate comprehensive strategy recommendations
        strategy_recommendations = await self.agent.recommend_strategy_updates(
            latest_effectiveness
        )

        print(f"\n‚úÖ **STRATEGY RECOMMENDATIONS GENERATED**:")
        print(
            f"‚Ä¢ Analysis Timestamp: {strategy_recommendations['analysis_timestamp'][:19]}"
        )
        print(f"‚Ä¢ Overall Assessment: {strategy_recommendations['overall_assessment']}")
        print(
            f"‚Ä¢ Recommendation Confidence: {strategy_recommendations['confidence_level']:.2f}"
        )

        # Show immediate actions
        immediate_actions = strategy_recommendations["immediate_actions"]
        if immediate_actions:
            print(f"\n‚ö° **IMMEDIATE ACTIONS REQUIRED**:")
            for action in immediate_actions:
                print(f"   ‚Ä¢ {action}")

        # Show method recommendations
        method_recommendations = strategy_recommendations["method_recommendations"]
        if method_recommendations:
            print(f"\nüî¨ **METHOD-SPECIFIC RECOMMENDATIONS**:")
            for method, rec in method_recommendations.items():
                print(f"   {method.title()}:")
                print(f"      ‚Ä¢ Action: {rec['action']}")
                print(f"      ‚Ä¢ Justification: {rec['justification']}")
                if "alternatives" in rec:
                    print(f"      ‚Ä¢ Alternatives: {', '.join(rec['alternatives'])}")
                if "improvements" in rec:
                    print(f"      ‚Ä¢ Improvements: {', '.join(rec['improvements'])}")
                if "expansion_suggestions" in rec:
                    print(
                        f"      ‚Ä¢ Expansion: {', '.join(rec['expansion_suggestions'])}"
                    )

        # Show agent recommendations
        agent_recommendations = strategy_recommendations["agent_recommendations"]
        if agent_recommendations:
            print(f"\nü§ñ **AGENT-SPECIFIC RECOMMENDATIONS**:")
            for agent, rec in agent_recommendations.items():
                print(f"   {agent}:")
                print(f"      ‚Ä¢ Action: {rec['action']}")
                print(f"      ‚Ä¢ Justification: {rec['justification']}")
                if "alternatives" in rec:
                    print(f"      ‚Ä¢ Alternatives: {', '.join(rec['alternatives'])}")
                if "configuration_changes" in rec:
                    print(
                        f"      ‚Ä¢ Config Changes: {', '.join(rec['configuration_changes'])}"
                    )
                if "priority_scenarios" in rec:
                    print(
                        f"      ‚Ä¢ Priority Scenarios: {', '.join(rec['priority_scenarios'])}"
                    )

        # Show escalation triggers
        escalation_triggers = strategy_recommendations["escalation_triggers"]
        if escalation_triggers:
            print(f"\nüö® **ESCALATION TRIGGERS IDENTIFIED**:")
            for trigger in escalation_triggers:
                print(f"   ‚Ä¢ {trigger}")
        else:
            print(
                f"\n‚úÖ **NO ESCALATION REQUIRED** - Performance within acceptable parameters"
            )

        # Show optimization opportunities
        optimization_opportunities = strategy_recommendations[
            "optimization_opportunities"
        ]
        if optimization_opportunities:
            print(f"\nüöÄ **OPTIMIZATION OPPORTUNITIES**:")
            for opportunity in optimization_opportunities:
                print(f"   ‚Ä¢ {opportunity}")

        # Show strategic changes
        strategic_changes = strategy_recommendations["strategic_changes"]
        if strategic_changes:
            print(f"\nüé≠ **STRATEGIC CHANGES RECOMMENDED**:")
            for change in strategic_changes:
                print(f"   ‚Ä¢ {change}")

        self.demo_results["strategy_recommendations"].append(strategy_recommendations)
        return strategy_recommendations

    async def demonstrate_adaptive_insights(self):
        """Show adaptive learning insights and meta-cognitive awareness"""
        print("\nüß† PHASE 4: ADAPTIVE LEARNING INSIGHTS")
        print("=" * 60)

        # Check if meta-learning history exists
        if hasattr(self.agent, "learning_sessions") and self.agent.learning_sessions:
            sessions = list(self.agent.learning_sessions.values())

            print(f"üß† **META-COGNITIVE ANALYSIS**:")
            print(f"‚Ä¢ Total Learning Sessions: {len(sessions)}")

            # Analyze learning patterns
            success_levels = [s["key_metrics"]["success_level"] for s in sessions]
            methods_used = [s["key_metrics"]["learning_method"] for s in sessions]
            agents_used = [s["key_metrics"]["agent_used"] for s in sessions]

            print(f"‚Ä¢ Success Distribution:")
            for level in ["breakthrough", "success", "partial", "failure"]:
                count = success_levels.count(level)
                percentage = (
                    (count / len(success_levels)) * 100 if success_levels else 0
                )
                print(f"    - {level.title()}: {count} ({percentage:.1f}%)")

            print(f"‚Ä¢ Method Usage:")
            unique_methods = list(set(methods_used))
            for method in unique_methods:
                count = methods_used.count(method)
                percentage = (count / len(methods_used)) * 100 if methods_used else 0
                print(f"    - {method.title()}: {count} ({percentage:.1f}%)")

            print(f"‚Ä¢ Agent Utilization:")
            unique_agents = list(set(agents_used))
            for agent in unique_agents:
                count = agents_used.count(agent)
                percentage = (count / len(agents_used)) * 100 if agents_used else 0
                print(f"    - {agent}: {count} ({percentage:.1f}%)")

            # Calculate learning velocity
            total_confidence_change = sum(
                s["key_metrics"]["confidence_change"] for s in sessions
            )
            total_time = sum(s["time_spent_minutes"] for s in sessions)
            avg_velocity = (
                total_confidence_change / (total_time / 60.0) if total_time > 0 else 0.0
            )

            print(f"‚Ä¢ Average Learning Velocity: {avg_velocity:.3f} confidence/hour")
            print(f"‚Ä¢ Total Learning Time: {total_time:.1f} minutes")
            print(f"‚Ä¢ Total Confidence Gained: {total_confidence_change:+.1f}")

        # Show adaptive threshold information
        if hasattr(self.agent, "adaptive_thresholds"):
            thresholds = self.agent.adaptive_thresholds
            print(f"\nüéõÔ∏è **ADAPTIVE THRESHOLD STATUS**:")
            print(
                f"‚Ä¢ Gap Detection Sensitivity: {thresholds.get('gap_detection_sensitivity', 0.5):.2f}"
            )
            print(
                f"‚Ä¢ Contradiction Severity Threshold: {thresholds.get('contradiction_severity_threshold', 0.7):.2f}"
            )
            print(
                f"‚Ä¢ Learning Goal Creation Rate: {thresholds.get('learning_goal_creation_rate', 0.8):.2f}"
            )
            print(
                f"‚Ä¢ Success Rate Target: {thresholds.get('success_rate_target', 0.75):.2f}"
            )

        print(f"\nüí° **ADAPTIVE LEARNING INSIGHTS**:")
        print(f"‚Ä¢ Meta-learning tracker successfully capturing learning patterns")
        print(f"‚Ä¢ Effectiveness scoring enabling continuous improvement")
        print(f"‚Ä¢ Strategy recommendations providing actionable guidance")
        print(f"‚Ä¢ System demonstrating self-awareness of learning performance")

    async def generate_meta_learning_report(self):
        """Generate comprehensive Meta-Learning Tracker report"""
        print("\nüìä META-LEARNING TRACKER COMPREHENSIVE REPORT")
        print("=" * 70)

        # Calculate metrics
        total_sessions = len(self.demo_results["learning_sessions"])
        total_effectiveness_analyses = len(self.demo_results["effectiveness_scores"])
        total_strategy_recommendations = len(
            self.demo_results["strategy_recommendations"]
        )

        # Calculate average effectiveness
        avg_effectiveness = 0.0
        if self.demo_results["effectiveness_scores"]:
            effectiveness_scores = [
                analysis["overall_effectiveness"]
                for analysis in self.demo_results["effectiveness_scores"]
            ]
            avg_effectiveness = sum(effectiveness_scores) / len(effectiveness_scores)

        # Method and success level analysis
        success_levels = [
            session["success_level"]
            for session in self.demo_results["learning_sessions"]
        ]
        methods_used = [
            session["method"] for session in self.demo_results["learning_sessions"]
        ]

        print(f"üìö **SESSION LOGGING PERFORMANCE**")
        print(f"‚Ä¢ Total Sessions Logged: {total_sessions}")
        print(f"‚Ä¢ Success Level Distribution:")
        for level in ["breakthrough", "success", "partial", "failure"]:
            count = success_levels.count(level)
            percentage = (count / len(success_levels)) * 100 if success_levels else 0
            print(f"    - {level.title()}: {count} ({percentage:.1f}%)")
        print(f"‚Ä¢ Learning Methods Used:")
        unique_methods = list(set(methods_used))
        for method in unique_methods:
            count = methods_used.count(method)
            print(f"    - {method.title()}: {count} sessions")

        print(f"\nüìä **EFFECTIVENESS SCORING PERFORMANCE**")
        print(f"‚Ä¢ Effectiveness Analyses: {total_effectiveness_analyses}")
        print(f"‚Ä¢ Average Learning Effectiveness: {avg_effectiveness:.2f}")
        print(
            f"‚Ä¢ Effectiveness Grade: {self.agent._grade_effectiveness(avg_effectiveness) if hasattr(self.agent, '_grade_effectiveness') else 'CALCULATED'}"
        )
        print(
            f"‚Ä¢ Multi-dimensional Analysis: ‚úÖ Confidence, Contradictions, Narrative Clarity"
        )

        print(f"\nüéØ **STRATEGY RECOMMENDATION PERFORMANCE**")
        print(f"‚Ä¢ Strategy Sessions: {total_strategy_recommendations}")
        print(
            f"‚Ä¢ Recommendation Categories: Method optimization, Agent configuration, Escalation management"
        )
        print(
            f"‚Ä¢ Adaptive Capability: ‚úÖ Dynamic threshold adjustment based on performance"
        )

        print(f"\nüß† **KEY META-LEARNING CAPABILITIES DEMONSTRATED**")
        print(
            f"‚Ä¢ ‚úÖ log_learning_session(): Comprehensive session logging with meta-analysis"
        )
        print(
            f"‚Ä¢ ‚úÖ score_effectiveness(): Multi-dimensional effectiveness scoring and grading"
        )
        print(
            f"‚Ä¢ ‚úÖ recommend_strategy_updates(): Intelligent strategy optimization suggestions"
        )
        print(f"‚Ä¢ ‚úÖ Method performance analysis with success rate tracking")
        print(f"‚Ä¢ ‚úÖ Agent effectiveness evaluation with configuration recommendations")
        print(f"‚Ä¢ ‚úÖ Escalation trigger identification for critical performance issues")
        print(f"‚Ä¢ ‚úÖ Adaptive threshold management for continuous optimization")

        print(f"\nüîÑ **META-LEARNING EVALUATION CYCLE**")
        print(
            f"‚Ä¢ Learning Session ‚Üí Effectiveness Analysis ‚Üí Strategy Recommendations ‚Üí Implementation"
        )
        print(f"‚Ä¢ Continuous adaptation based on learning performance metrics")
        print(f"‚Ä¢ Self-awareness of learning capabilities and limitations")
        print(f"‚Ä¢ Autonomous optimization of learning strategies and approaches")

        # Save comprehensive report
        report_data = {
            "demo_timestamp": datetime.now().isoformat(),
            "system_status": "Meta-Learning Tracker Operational",
            "session_logging": {
                "total_sessions": total_sessions,
                "success_distribution": {
                    level: success_levels.count(level)
                    for level in ["breakthrough", "success", "partial", "failure"]
                },
                "method_usage": {
                    method: methods_used.count(method) for method in unique_methods
                },
            },
            "effectiveness_scoring": {
                "analyses_performed": total_effectiveness_analyses,
                "average_effectiveness": avg_effectiveness,
                "scoring_capability": "multi_dimensional",
            },
            "strategy_recommendations": {
                "recommendation_sessions": total_strategy_recommendations,
                "adaptive_capability": True,
                "optimization_categories": ["method", "agent", "escalation"],
            },
            "demo_results": self.demo_results,
        }

        # Save report
        report_file = Path("meta_learning_tracker_report.json")
        with open(report_file, "w") as f:
            json.dump(report_data, f, indent=2)

        print(f"\nüíæ Detailed report saved to: {report_file}")
        print(f"\nüéâ **META-LEARNING TRACKER: FULLY OPERATIONAL!**")


async def main():
    """Run the Meta-Learning Tracker demonstration"""
    demo = MetaLearningTrackerDemo()

    try:
        await demo.run_comprehensive_demo()

        print("\n" + "=" * 70)
        print("üèÅ META-LEARNING TRACKER DEMO COMPLETED SUCCESSFULLY!")
        print("‚úÖ Comprehensive learning session logging and analysis")
        print("‚úÖ Multi-dimensional effectiveness scoring and grading")
        print("‚úÖ Intelligent strategy recommendations with escalation triggers")
        print("‚úÖ Complete meta-learning evaluation and optimization cycle")
        print("=" * 70)

    except Exception as e:
        print(f"\n‚ùå Demo encountered an error: {e}")
        print("üîß This indicates integration issues that need to be resolved")
        raise


if __name__ == "__main__":
    # Run the Meta-Learning Tracker demonstration
    asyncio.run(main())
